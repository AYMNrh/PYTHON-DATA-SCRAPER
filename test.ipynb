{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\",\"Page\",\"Time\",\"Time taken\",\"Time taken formatted\"]\n",
    "\n",
    "# Create a directory to store the files\n",
    "directory = \"threads\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Define a function to fetch the data from a page\n",
    "def fetch_data(page_number):\n",
    "    url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber={}'.format(page_number)\n",
    "    response = requests.get(url)\n",
    "    # soup = BeautifulSoup(response.content, 'lxml')\n",
    "    # soup = BeautifulSoup(response.content.decode('UTF-8'), 'lxml')\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    all_tables = soup.find_all('table', {'class': 'bordertb'})\n",
    "    if len(all_tables) >= 2:\n",
    "        outer_table = all_tables[1]\n",
    "        inner_table = outer_table.find('table', {'id': 'tblTransactionSearchResult'})\n",
    "        if inner_table is not None:\n",
    "            rows = []\n",
    "            for row in inner_table.find_all('tr')[2:]:\n",
    "                cells = row.find_all('td')\n",
    "                rows.append([val.text.strip() for val in cells[:len(cells)-1]])\n",
    "            rows = [row for row in rows if any(cell.strip() != \"\" for cell in row)]\n",
    "            return rows\n",
    "    return None\n",
    "start_time = time.time()\n",
    "# Create a thread pool with 16 workers\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             df = pd.DataFrame(result, columns=headers)\n",
    "\n",
    "#             # Save the data to a file\n",
    "#             file_name = f\"transactions/transactions_{i+1}.csv\"\n",
    "#             df.to_csv(file_name, index=False)\n",
    "\n",
    "# # Merge all the files into a single CSV file\n",
    "# merged_file = f\"transactions/merged_transactions.csv\"\n",
    "# df = pd.concat([pd.read_csv(f) for f in os.listdir(directory) if f.endswith('.csv')], ignore_index=True)\n",
    "# df.to_csv(merged_file, index=False)\n",
    "\n",
    "# print(\"I am done scraping the data and merged the files into a single CSV file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concurrent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Use the map function to fetch the data for each page number in parallel   \u001b[39;00m\n\u001b[0;32m      5\u001b[0m     results \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mmap(fetch_data, \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m,\u001b[39m10\u001b[39m)) \u001b[39m#0-4230-8460-12690-16920-21150-25380-29610-33840-38070-42300-46530-50760-55342\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39m# print results type   \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'concurrent' is not defined"
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    # Use the map function to fetch the data for each page number in parallel   \n",
    "    \n",
    "\n",
    "    results = executor.map(fetch_data, range(2,10)) #0-4230-8460-12690-16920-21150-25380-29610-33840-38070-42300-46530-50760-55342\n",
    "    # print results type   \n",
    "    print(type(results))\n",
    "# results is a data frame, so we can iterate over it and fuse each result to a dataframe\n",
    "    #create dataframe\n",
    "    finaldf = pd.DataFrame()\n",
    "    for i, result in enumerate(results):\n",
    "        df = pd.DataFrame(results, columns=headers) \n",
    "        file_name = f\"{directory}/transactions_{i}.csv\"\n",
    "        finaldf = pd.concat([finaldf,df])\n",
    "    finaldf.to_csv('finaldest1.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge all the files into a single CSV file\n",
    "# merged_file = f\"{directory}/merged_transactions.csv\"\n",
    "# with open(merged_file, 'w') as outfile:\n",
    "#     for file_name in os.listdir(directory):\n",
    "#         if file_name.startswith(\"transactions_\"):\n",
    "#             with open(f\"{directory}/{file_name}\") as infile:\n",
    "#                 outfile.write(infile.read())\n",
    "\n",
    "end_time = time.time()\n",
    "# total_time = end_time - start_time\n",
    "time_taken = end_time - start_time\n",
    "time_taken_formatted = str(datetime.timedelta(seconds=time_taken))\n",
    "print(\"Time taken: {}\".format(time_taken_formatted))\n",
    "\n",
    "\n",
    "print(\"I am done scraping the data and merged it into a single file\")\n",
    "\n",
    "df.head()\n",
    "# df.to_csv('thread1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1562161071.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    last_page = needs to be scraped\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\"]\n",
    "\n",
    "# Read the existing transaction IDs from the previously scraped CSV file\n",
    "# df_existing = pd.read_csv('TransactionsSansVerif.csv')\n",
    "df_existing = pd.read_csv('Transactions55K.csv')\n",
    "existing_ids = set(df_existing['Transaction ID'].unique())\n",
    "print(\"Found {} existing transaction IDs\".format(len(existing_ids)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# number_of_ids = 58\n",
    "print(\"Number of IDs: {}\".format(number_of_ids))\n",
    "number_of_pages = number_of_ids//20\n",
    "print(\"Number of pages: {}\".format(number_of_pages))\n",
    "first_page = number_of_pages \n",
    "\n",
    "# last_page = number_of_pages - 55320\n",
    "\n",
    "\n",
    "# for i in range(0, last_page):\n",
    "# \tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last page: 55342\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# first_page = 2\n",
    "# last_page = 6\n",
    "\n",
    "# check how many pages from file to determine where to start scraping\n",
    "# range is now (first_page, last_page)\n",
    "# url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber=1'\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.content, 'lxml')\n",
    "# # Get the value of the resultList.lastPageNumber input\n",
    "# number_of_ids = len(existing_ids)\n",
    "# number_of_pages = number_of_ids//20\n",
    "# first_page = number_of_pages\n",
    "# print(\"First page: {}\".format(first_page))\n",
    "# last_page = int(soup.find('input', {'name': 'resultList.lastPageNumber'}).get('value'))\n",
    "# print(\"Last page: {}\".format(last_page))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 existing transaction IDs\n",
      "First page: 6\n",
      "Last page: 55342\n"
     ]
    }
   ],
   "source": [
    "# Read the existing transaction IDs from the previously scraped CSV file\n",
    "try:\n",
    "\tdf_existing = pd.read_csv('TransactionsSansVerif.csv')\n",
    "\t# df_existing = pd.read_csv('Transactions55K.csv')\n",
    "\texisting_ids = set(df_existing['Transaction ID'].unique())\n",
    "\tprint(\"Found {} existing transaction IDs\".format(len(existing_ids)))\n",
    "except FileNotFoundError:\n",
    "\texisting_ids = set()\n",
    "\tprint(\"No existing transaction IDs found\")\n",
    "\n",
    "\n",
    "\n",
    "# check how many pages from file to determine where to start scraping\n",
    "# range is now (first_page, last_page)\n",
    "url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber=1'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "# Get the value of the resultList.lastPageNumber input\n",
    "number_of_ids = len(existing_ids)\n",
    "number_of_pages = number_of_ids//20\n",
    "first_page = number_of_pages\n",
    "print(\"First page: {}\".format(first_page))\n",
    "last_page = int(soup.find('input', {'name': 'resultList.lastPageNumber'}).get('value'))\n",
    "print(\"Last page: {}\".format(last_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0, 5):\n",
    "\tprint(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbce1745e8b4a8130c10f0a2680b2623aa1937f6f74daff15e9c3ceecef110d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
