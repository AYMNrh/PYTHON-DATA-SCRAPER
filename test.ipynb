{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\"]\n",
    "\n",
    "# Create a directory to store the files\n",
    "directory = \"transactions\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Define a function to fetch the data from a page\n",
    "def fetch_data(page_number):\n",
    "    url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber={}'.format(page_number)\n",
    "    response = requests.get(url)\n",
    "    # soup = BeautifulSoup(response.content, 'lxml')\n",
    "    # soup = BeautifulSoup(response.content.decode('UTF-8'), 'lxml')\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    all_tables = soup.find_all('table', {'class': 'bordertb'})\n",
    "    if len(all_tables) >= 2:\n",
    "        outer_table = all_tables[1]\n",
    "        inner_table = outer_table.find('table', {'id': 'tblTransactionSearchResult'})\n",
    "        if inner_table is not None:\n",
    "            rows = []\n",
    "            for row in inner_table.find_all('tr')[2:]:\n",
    "                cells = row.find_all('td')\n",
    "                rows.append([val.text.strip() for val in cells[:len(cells)-1]])\n",
    "            rows = [row for row in rows if any(cell.strip() != \"\" for cell in row)]\n",
    "            return rows\n",
    "    return None\n",
    "start_time = time.time()\n",
    "# Create a thread pool with 16 workers\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             df = pd.DataFrame(result, columns=headers)\n",
    "\n",
    "#             # Save the data to a file\n",
    "#             file_name = f\"transactions/transactions_{i+1}.csv\"\n",
    "#             df.to_csv(file_name, index=False)\n",
    "\n",
    "# # Merge all the files into a single CSV file\n",
    "# merged_file = f\"transactions/merged_transactions.csv\"\n",
    "# df = pd.concat([pd.read_csv(f) for f in os.listdir(directory) if f.endswith('.csv')], ignore_index=True)\n",
    "# df.to_csv(merged_file, index=False)\n",
    "\n",
    "# print(\"I am done scraping the data and merged the files into a single CSV file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "16 columns passed, passed data had 20 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 16 columns passed, passed data had 20 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(results))\n\u001b[0;32m      6\u001b[0m \u001b[39m# save results into csv file\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(results, columns\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Iterate over the results and save the data to a file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#create dataframe\u001b[39;00m\n\u001b[0;32m     13\u001b[0m finaldf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:745\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    744\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 745\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    746\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    747\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    748\u001b[0m         data,\n\u001b[0;32m    749\u001b[0m         columns,\n\u001b[0;32m    750\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    751\u001b[0m         dtype,\n\u001b[0;32m    752\u001b[0m     )\n\u001b[0;32m    753\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    754\u001b[0m         arrays,\n\u001b[0;32m    755\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    758\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    759\u001b[0m     )\n\u001b[0;32m    760\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 510\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    511\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    513\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 16 columns passed, passed data had 20 columns"
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    # Use the map function to fetch the data for each page number in parallel\n",
    "    results = executor.map(fetch_data, range(2,22)) #0-4230-8460-12690-16920-21150-25380-29610-33840-38070-42300-46530-50760-55342\n",
    "    # print results type   \n",
    "    print(type(results))\n",
    "\n",
    "    \n",
    "    # Iterate over the results and save the data to a file\n",
    "    #create dataframe\n",
    "    finaldf = pd.DataFrame()\n",
    "    for i, result in enumerate(results):\n",
    "        df = pd.DataFrame(results, columns=headers)\n",
    "        finaldf = pd.concat([finaldf,df])\n",
    "    finaldf.to_csv('finaldest1.csv', index=False) \n",
    "  \n",
    "# Merge all the files into a single CSV file\n",
    "# merged_file = f\"{directory}/merged_transactions.csv\"\n",
    "# with open(merged_file, 'w') as outfile:\n",
    "#     for file_name in os.listdir(directory):\n",
    "#         if file_name.startswith(\"transactions_\"):\n",
    "#             with open(f\"{directory}/{file_name}\") as infile:\n",
    "#                 outfile.write(infile.read())\n",
    "\n",
    "end_time = time.time()\n",
    "# total_time = end_time - start_time\n",
    "time_taken = end_time - start_time\n",
    "time_taken_formatted = str(datetime.timedelta(seconds=time_taken))\n",
    "print(\"Time taken: {}\".format(time_taken_formatted))\n",
    "\n",
    "\n",
    "print(\"I am done scraping the data and merged it into a single file\")\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbce1745e8b4a8130c10f0a2680b2623aa1937f6f74daff15e9c3ceecef110d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
