{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\"]\n",
    "\n",
    "# Create a directory to store the files\n",
    "directory = \"transactions\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Define a function to fetch the data from a page\n",
    "def fetch_data(page_number):\n",
    "    url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber={}'.format(page_number)\n",
    "    response = requests.get(url)\n",
    "    # soup = BeautifulSoup(response.content, 'lxml')\n",
    "    # soup = BeautifulSoup(response.content.decode('UTF-8'), 'lxml')\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    all_tables = soup.find_all('table', {'class': 'bordertb'})\n",
    "    if len(all_tables) >= 2:\n",
    "        outer_table = all_tables[1]\n",
    "        inner_table = outer_table.find('table', {'id': 'tblTransactionSearchResult'})\n",
    "        if inner_table is not None:\n",
    "            rows = []\n",
    "            for row in inner_table.find_all('tr')[2:]:\n",
    "                cells = row.find_all('td')\n",
    "                rows.append([val.text.strip() for val in cells[:len(cells)-1]])\n",
    "            rows = [row for row in rows if any(cell.strip() != \"\" for cell in row)]\n",
    "            return rows\n",
    "    return None\n",
    "start_time = time.time()\n",
    "# Create a thread pool with 16 workers\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             df = pd.DataFrame(result, columns=headers)\n",
    "\n",
    "#             # Save the data to a file\n",
    "#             file_name = f\"transactions/transactions_{i+1}.csv\"\n",
    "#             df.to_csv(file_name, index=False)\n",
    "\n",
    "# # Merge all the files into a single CSV file\n",
    "# merged_file = f\"transactions/merged_transactions.csv\"\n",
    "# df = pd.concat([pd.read_csv(f) for f in os.listdir(directory) if f.endswith('.csv')], ignore_index=True)\n",
    "# df.to_csv(merged_file, index=False)\n",
    "\n",
    "# print(\"I am done scraping the data and merged the files into a single CSV file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "Time taken: 0:00:17.518375\n",
      "I am done scraping the data and merged it into a single file\n"
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    # Use the map function to fetch the data for each page number in parallel\n",
    "    results = executor.map(fetch_data, range(2,22)) #0-4230-8460-12690-16920-21150-25380-29610-33840-38070-42300-46530-50760-55342\n",
    "    # print results type   \n",
    "    print(type(results))\n",
    "\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        if result is not None:\n",
    "            df = pd.DataFrame(result, columns=headers)\n",
    "            file_name = f\"{directory}/transactions_1.csv\"\n",
    "            df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Iterate over the results and save the data to a file\n",
    "    #create dataframe\n",
    "    # finaldf = pd.DataFrame()\n",
    "    # for i, result in enumerate(results):\n",
    "    #     df = pd.DataFrame(results, columns=headers)\n",
    "    #     finaldf = pd.concat([finaldf,df])\n",
    "    #     file_name = f\"simulta1.csv\"\n",
    "    #     df.to_csv(file_name, index=False)\n",
    "    # finaldf.to_csv('simulta1.csv', index=False) \n",
    "  \n",
    "# Merge all the files into a single CSV file\n",
    "# merged_file = f\"{directory}/merged_transactions.csv\"\n",
    "# with open(merged_file, 'w') as outfile:\n",
    "#     for file_name in os.listdir(directory):\n",
    "#         if file_name.startswith(\"transactions_\"):\n",
    "#             with open(f\"{directory}/{file_name}\") as infile:\n",
    "#                 outfile.write(infile.read())\n",
    "\n",
    "end_time = time.time()\n",
    "# total_time = end_time - start_time\n",
    "time_taken = end_time - start_time\n",
    "time_taken_formatted = str(datetime.timedelta(seconds=time_taken))\n",
    "print(\"Time taken: {}\".format(time_taken_formatted))\n",
    "\n",
    "\n",
    "print(\"I am done scraping the data and merged it into a single file\")\n",
    "\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b82bba91dfa0b32476058284f3c3da4e948f9193da329786141cd69a71bddd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
