{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 existing transaction IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m row_data \u001b[39m=\u001b[39m [val\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m cells[:\u001b[39mlen\u001b[39m(cells)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[0;32m     42\u001b[0m \u001b[39m# Check if the transaction ID is already in the existing IDs set\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[39mif\u001b[39;00m row_data[\u001b[39m0\u001b[39;49m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m existing_ids:\n\u001b[0;32m     44\u001b[0m     rows\u001b[39m.\u001b[39mappend(row_data)\n\u001b[0;32m     45\u001b[0m     existing_ids\u001b[39m.\u001b[39madd(row_data[\u001b[39m0\u001b[39m]) \u001b[39m# Add the new ID to the existing IDs set\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\"]\n",
    "\n",
    "# Read the existing transaction IDs from the previously scraped CSV file\n",
    "try:\n",
    "    df_existing = pd.read_csv('TransactionsFinal100pages.csv')\n",
    "    existing_ids = set(df_existing['Transaction ID'].unique())\n",
    "    print(\"Found {} existing transaction IDs\".format(len(existing_ids)))\n",
    "except FileNotFoundError:\n",
    "    existing_ids = set()\n",
    "    print(\"No existing transaction IDs found\")\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "data = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through the pages\n",
    "for i in tqdm(range(2, 22)):\n",
    "    # Check if the page has already been scraped by checking if all transaction IDs on the page are in the existing IDs set\n",
    "    url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber={}'.format(i)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    all_tables = soup.find_all('table', {'class': 'bordertb'})\n",
    "\n",
    "    # Check if the inner table exists\n",
    "    if len(all_tables) >= 2:\n",
    "        outer_table = all_tables[1]\n",
    "        inner_table = outer_table.find('table', {'id': 'tblTransactionSearchResult'})\n",
    "        if inner_table is not None:\n",
    "            rows = []\n",
    "            for row in inner_table.find_all('tr')[2:]:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [val.text.strip() for val in cells[:len(cells)-1]]\n",
    "                # Check if the transaction ID is already in the existing IDs set\n",
    "                if row_data[0] not in existing_ids:\n",
    "                    rows.append(row_data)\n",
    "                    existing_ids.add(row_data[0]) # Add the new ID to the existing IDs set\n",
    "            rows = [row for row in rows if any(cell.strip() != \"\" for cell in row)]\n",
    "            data.append(rows)\n",
    "    else:\n",
    "        print(\"Table not found\")\n",
    "\n",
    "# Concatenate the list into a DataFrame\n",
    "df = pd.concat([pd.DataFrame(i, columns=headers) for i in data], ignore_index=True)\n",
    "\n",
    "# Append the new data to the existing CSV file\n",
    "df.to_csv('TransactionsFinal100pages.csv', index=False, mode='a', header=not os.path.exists('TransactionsFinal100pages.csv'))\n",
    "\n",
    "print(\"I am done scraping the data\")\n",
    "end_time = time.time()\n",
    "time_taken = end_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 existing transaction IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "16 columns passed, passed data had 1 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\CYTech Student\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CYTech Student\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 16 columns passed, passed data had 1 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTable not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[39m# Create a DataFrame from the list of rows\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data, columns\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m     54\u001b[0m \u001b[39m# Append the new data to the existing CSV file\u001b[39;00m\n\u001b[0;32m     55\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mTransactionsFinal100pages.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m'\u001b[39m\u001b[39mTransactionsFinal100pages.csv\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\CYTech Student\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:745\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    744\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 745\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    746\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    747\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    748\u001b[0m         data,\n\u001b[0;32m    749\u001b[0m         columns,\n\u001b[0;32m    750\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    751\u001b[0m         dtype,\n\u001b[0;32m    752\u001b[0m     )\n\u001b[0;32m    753\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    754\u001b[0m         arrays,\n\u001b[0;32m    755\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    758\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    759\u001b[0m     )\n\u001b[0;32m    760\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\CYTech Student\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 510\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    511\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    513\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\CYTech Student\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Users\\CYTech Student\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 16 columns passed, passed data had 1 columns"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\"]\n",
    "\n",
    "# Read the existing transaction IDs from the previously scraped CSV file\n",
    "try:\n",
    "    df_existing = pd.read_csv('TransactionsFinal100pages.csv')\n",
    "    existing_ids = set(df_existing['Transaction ID'].unique())\n",
    "    print(\"Found {} existing transaction IDs\".format(len(existing_ids)))\n",
    "except FileNotFoundError:\n",
    "    existing_ids = set()\n",
    "    print(\"No existing transaction IDs found\")\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "data = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through the pages\n",
    "for i in tqdm(range(2, 22)):\n",
    "    # Check if the page has already been scraped by checking if all transaction IDs on the page are in the existing IDs set\n",
    "    url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber={}'.format(i)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    all_tables = soup.find_all('table', {'class': 'bordertb'})\n",
    "\n",
    "    # Check if the inner table exists\n",
    "    if len(all_tables) >= 2:\n",
    "        outer_table = all_tables[1]\n",
    "        inner_table = outer_table.find('table', {'id': 'tblTransactionSearchResult'})\n",
    "        if inner_table is not None:\n",
    "            rows = []\n",
    "            for row in inner_table.find_all('tr')[2:]:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [val.text.strip() for val in cells]\n",
    "                # Check if the transaction ID is already in the existing IDs set\n",
    "                if row_data and row_data[0] not in existing_ids:\n",
    "                    rows.append(row_data)\n",
    "                    existing_ids.add(row_data[0]) # Add the new ID to the existing IDs set\n",
    "            data.extend(rows)\n",
    "    else:\n",
    "        print(\"Table not found\")\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Append the new data to the existing CSV file\n",
    "df.to_csv('TransactionsFinal100pages.csv', index=False, mode='a', header=not os.path.exists('TransactionsFinal100pages.csv'))\n",
    "\n",
    "print(\"I am done scraping the data\")\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken: {:.2f} seconds\".format(time_taken))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 existing transaction IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m row_data \u001b[39m=\u001b[39m [val\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m cells[:\u001b[39mlen\u001b[39m(cells)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[0;32m     44\u001b[0m \u001b[39m# Check if the transaction ID is already in the existing IDs set\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mif\u001b[39;00m row_data[\u001b[39m0\u001b[39;49m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m existing_ids:\n\u001b[0;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(row_data) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(headers):\n\u001b[0;32m     47\u001b[0m         rows\u001b[39m.\u001b[39mappend(row_data)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the headers\n",
    "headers = [\"Transaction ID\", \"Transaction Type\", \"Transaction Date\", \"Transaction Status\", \"Transferring Registry\", \"Transferring Account Type\", \"Transferring Account Name\", \"Transferring Account Identifier\", \"Transferring Account Holder\", \"Acquiring Registry\", \"Acquiring Account Type\", \"Acquiring Account Name\", \"Acquiring Account Identifier\", \"Acquiring Account Holder\",\"Nb of units\",\"Option\"]\n",
    "\n",
    "# Read the existing transaction IDs from the previously scraped CSV file\n",
    "try:\n",
    "    df_existing = pd.read_csv('TransactionsFinal100pages.csv')\n",
    "    existing_ids = set(df_existing['Transaction ID'].unique())\n",
    "    print(\"Found {} existing transaction IDs\".format(len(existing_ids)))\n",
    "except FileNotFoundError:\n",
    "    existing_ids = set()\n",
    "    print(\"No existing transaction IDs found\")\n",
    "\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "data = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through the pages\n",
    "for i in tqdm(range(2, 22)):\n",
    "    # Check if the page has already been scraped by checking if all transaction IDs on the page are in the existing IDs set\n",
    "    url = 'https://ec.europa.eu/clima/ets/transaction.do?languageCode=fr&startDate=&endDate=&transactionStatus=4&fromCompletionDate=&toCompletionDate=&transactionID=&transactionType=-1&suppTransactionType=-1&originatingRegistry=-1&destinationRegistry=-1&originatingAccountType=-1&destinationAccountType=-1&originatingAccountIdentifier=&destinationAccountIdentifier=&originatingAccountHolder=&destinationAccountHolder=&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber={}'.format(i)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    all_tables = soup.find_all('table', {'class': 'bordertb'})\n",
    "\n",
    "    # Check if the inner table exists\n",
    "    if len(all_tables) >= 2:\n",
    "        outer_table = all_tables[1]\n",
    "        inner_table = outer_table.find('table', {'id': 'tblTransactionSearchResult'})\n",
    "        if inner_table is not None:\n",
    "            rows = []\n",
    "            for row in inner_table.find_all('tr')[2:]:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [val.text.strip() for val in cells]\n",
    "                # Check if the transaction ID is already in the existing IDs set\n",
    "                if row_data and row_data[0] not in existing_ids:\n",
    "                    rows.append(row_data)\n",
    "                    existing_ids.add(row_data[0]) # Add the new ID to the existing IDs set\n",
    "            data.extend(rows)\n",
    "    else:\n",
    "        print(\"Table not found\")\n",
    "# Flatten the list of rows into a single list\n",
    "rows = [row for page in data for row in page]\n",
    "\n",
    "# Create a DataFrame from the rows\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Append the new data to the existing CSV file\n",
    "df.to_csv('TransactionsFinal100pages.csv', index=False, mode='a', header=not os.path.exists('TransactionsFinal100pages.csv'))\n",
    "\n",
    "print(\"I am done scraping the data\")\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken:\", str(datetime.timedelta(seconds=time_taken)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b82bba91dfa0b32476058284f3c3da4e948f9193da329786141cd69a71bddd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
